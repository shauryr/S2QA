{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.query_engine import CitationQueryEngine\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_hub.semanticscholar.base import SemanticScholarReader\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "from llama_index.utils import globals_helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "s2reader = SemanticScholarReader()\n",
    "\n",
    "# narrow down the search space\n",
    "query_space = \"biases in large language models\"\n",
    "\n",
    "# increase limit to get more documents\n",
    "documents = s2reader.load_data(query=query_space, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** | Source | Bias Type | Description |\n",
       "| --- | --- | --- |\n",
       "| 1 | Data Selection Bias | The bias caused by the choice of texts that make up a training corpus. |\n",
       "| 1 | Social Bias | Ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. |\n",
       "| 2 | Capturing Failures of Large Language Models via Human Cognitive Biases | Draw inspiration from human cognitive biases -- systematic patterns of deviation from rational judgement. |\n",
       "| 3 | Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models | Unintended consequences of biased model outputs, ethical concerns arising from the deployment of these models in various applications. |\n",
       "\n",
       "Note: The above table is a simplified representation of the biases present in large language models, as discussed in the provided sources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 66936407-3959-4dec-a375-2e5a2b3614c8<br>**Similarity:** 0.8527078275082337<br>**Text:** Source 1:\n",
       "Biases in Large Language Models: Origins, Inventory, and Discussion In this article, we...<br>**Metadata:** {'title': 'Biases in Large Language Models: Origins, Inventory, and Discussion', 'venue': 'ACM Journal of Data and Information Quality', 'year': 2023, 'paperId': '6d0656d9bb60a2bea50c4b894fbcc5d1e32134e7', 'citationCount': 4, 'openAccessPdf': 'https://dl.acm.org/doi/pdf/10.1145/3597307', 'authors': ['Roberto Navigli', 'Simone Conia', 'Björn Ross'], 'externalIds': {'DBLP': 'journals/jdiq/NavigliCR23', 'DOI': '10.1145/3597307', 'CorpusId': 258688053}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** aaecd734-cc0c-4e36-8a57-7edf21dc002f<br>**Similarity:** 0.8334321916861845<br>**Text:** Source 2:\n",
       "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models As the ...<br>**Metadata:** {'title': 'Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '16d83e930a4dab2d49f5d276838ddce79df3f787', 'citationCount': 37, 'openAccessPdf': None, 'authors': ['Emilio Ferrara'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-03738', 'ArXiv': '2304.03738', 'DOI': '10.48550/arXiv.2304.03738', 'CorpusId': 258041203}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 3/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 876df3ac-7fea-4c30-8c9a-df6029cc4853<br>**Similarity:** 0.8321528624249883<br>**Text:** Source 3:\n",
       "Capturing Failures of Large Language Models via Human Cognitive Biases Large language m...<br>**Metadata:** {'title': 'Capturing Failures of Large Language Models via Human Cognitive Biases', 'venue': 'Neural Information Processing Systems', 'year': 2022, 'paperId': '76f023c3a819fc58989a064a1b50825b11fce95d', 'citationCount': 30, 'openAccessPdf': None, 'authors': ['Erik Jones', 'J. Steinhardt'], 'externalIds': {'DBLP': 'journals/corr/abs-2202-12299', 'ArXiv': '2202.12299', 'CorpusId': 247084098}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "query_engine_default = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    citation_chunk_size=512,\n",
    ")\n",
    "\n",
    "# query the index\n",
    "query_string = \"explain all the biases in large language models in a markdown table\"\n",
    "# query the citation query engine\n",
    "response = query_engine_default.query(query_string)\n",
    "display_response(\n",
    "    response, show_source=True, source_length=100, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Sure, here's a markdown table summarizing the biases in large language models discussed in the three sources:\n",
       "\n",
       "| Source | Biases in Large Language Models |\n",
       "| --- | --- |\n",
       "| Source 1 | Bias due to unintended consequences of biased model outputs<br>Bias due to product design and policy decisions |\n",
       "| Source 2 | Moral biases (captured human-like moral norms) in monolingual English language models<br>Lack of capture of cultural diversity and shared tendencies across countries |\n",
       "| Source 3 | Anxiety-induced exploration and bias in GPT-3.5, including increased biases when prompted with anxiety-inducing text |\n",
       "\n",
       "In summary, the biases in large language models discussed in these sources can be categorized into three main types:\n",
       "\n",
       "1. Unintended consequences of biased model outputs, which can arise from various factors such as the nature of training data or product design and policy decisions.\n",
       "2. Moral biases, where language models capture human-like moral norms but lack capturing cultural diversity and shared tendencies across countries.\n",
       "3. Emotion-induced exploration and bias, where anxiety-inducing prompts can influence the behavior of large language models, leading to increased biases in certain tasks.\n",
       "\n",
       "These findings highlight the need for careful consideration when designing and deploying large language models, particularly in applied settings where their outputs may have significant consequences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** aaecd734-cc0c-4e36-8a57-7edf21dc002f<br>**Similarity:** 5.513845376078951<br>**Text:** Source 1:\n",
       "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models As the ...<br>**Metadata:** {'title': 'Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '16d83e930a4dab2d49f5d276838ddce79df3f787', 'citationCount': 37, 'openAccessPdf': None, 'authors': ['Emilio Ferrara'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-03738', 'ArXiv': '2304.03738', 'DOI': '10.48550/arXiv.2304.03738', 'CorpusId': 258041203}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 262976da-5596-41fe-90ad-84893f44dc78<br>**Similarity:** 5.39147178251072<br>**Text:** Source 2:\n",
       "Knowledge of cultural moral norms in large language models Moral norms vary across cult...<br>**Metadata:** {'title': 'Knowledge of cultural moral norms in large language models', 'venue': 'Annual Meeting of the Association for Computational Linguistics', 'year': 2023, 'paperId': '8ea24b1dbb3e690ebc64543c03f0552a6c1fb49d', 'citationCount': 3, 'openAccessPdf': None, 'authors': ['Aida Ramezani', 'Yang Xu'], 'externalIds': {'ArXiv': '2306.01857', 'ACL': '2023.acl-long.26', 'DBLP': 'conf/acl/Ramezani023', 'DOI': '10.48550/arXiv.2306.01857', 'CorpusId': 259075607}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 3/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 87df8718-ae81-4a09-ab37-8e37db84e241<br>**Similarity:** 5.2966932690268544<br>**Text:** Source 3:\n",
       "Inducing anxiety in large language models increases exploration and bias Large language...<br>**Metadata:** {'title': 'Inducing anxiety in large language models increases exploration and bias', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '27c16cca907aa43397cc226a182b73b396c5cf66', 'citationCount': 14, 'openAccessPdf': None, 'authors': ['Julian Coda-Forno', 'K. Witte', 'A. Jagadish', 'Marcel Binz', 'Zeynep Akata', 'Eric Schulz'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-11111', 'ArXiv': '2304.11111', 'DOI': '10.48550/arXiv.2304.11111', 'CorpusId': 258291914}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(index, similarity_top_k=3)\n",
    "query_engine_bm25 = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    retriever=bm25_retriever,\n",
    "    similarity_top_k=3,\n",
    "    citation_chunk_size=512,\n",
    ")\n",
    "\n",
    "# query the citation query engine\n",
    "response = query_engine_bm25.query(query_string)\n",
    "display_response(\n",
    "    response, show_source=True, source_length=100, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided sources, there are several methods proposed to mitigate biases in large language models:\n",
       "\n",
       "1. Data selection bias: This method involves selecting a diverse and representative dataset for training the language model, to reduce the likelihood of biased outputs [1].\n",
       "2. Fairness-aware training: This approach involves training the language model with fairness constraints, such as weighted loss functions that prioritize fairness over accuracy [3].\n",
       "3. Adversarial training: This method involves training the language model on adversarial examples generated using techniques like text perturbation, to reduce the model's sensitivity to biased inputs [2].\n",
       "4. Debiasing techniques: This category includes methods such as word embedding debiasing, which can be used to remove gender or racial stereotypes from word embeddings [7].\n",
       "5. Evaluation and monitoring: Regularly evaluating and monitoring the language model's outputs for biases can help identify and address any issues before they cause harm [6].\n",
       "6. Diverse and inclusive product design: Ensuring that the language model is designed with diverse and inclusive features can help reduce biases in the output [4].\n",
       "7. Algorithmic constraints: Implementing algorithmic constraints, such as fairness-aware sampling or adversarial training, can help mitigate biases in the language model's outputs [3].\n",
       "8. Model interpretability: Providing interpretable models that allow users to understand how the model is making predictions can help identify and address any biases [5].\n",
       "9. Regularization techniques: Using regularization techniques, such as weight decay or adversarial training, can help reduce biases in the language model's outputs [8].\n",
       "10. Human oversight: Implementing human oversight mechanisms, such as fact-checking or peer review, can help identify and address any biases in the language model's outputs [9].\n",
       "\n",
       "It is important to note that these methods are not mutually exclusive, and a combination of them may be necessary to effectively mitigate biases in large language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 66936407-3959-4dec-a375-2e5a2b3614c8<br>**Similarity:** 0.8588361576198662<br>**Text:** Source 1:\n",
       "Biases in Large Language Models: Origins, Inventory, and Discussion In this article, we...<br>**Metadata:** {'title': 'Biases in Large Language Models: Origins, Inventory, and Discussion', 'venue': 'ACM Journal of Data and Information Quality', 'year': 2023, 'paperId': '6d0656d9bb60a2bea50c4b894fbcc5d1e32134e7', 'citationCount': 4, 'openAccessPdf': 'https://dl.acm.org/doi/pdf/10.1145/3597307', 'authors': ['Roberto Navigli', 'Simone Conia', 'Björn Ross'], 'externalIds': {'DBLP': 'journals/jdiq/NavigliCR23', 'DOI': '10.1145/3597307', 'CorpusId': 258688053}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 670e58cd-d702-4a24-8a0d-85c14eed18e5<br>**Similarity:** 0.8509060499403998<br>**Text:** Source 2:\n",
       "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Languag...<br>**Metadata:** {'title': 'FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '785bd5915f83f941d36c7996a9742ae695880111', 'citationCount': 2, 'openAccessPdf': None, 'authors': ['Hrishikesh Viswanath', 'Tianyi Zhang'], 'externalIds': {'ArXiv': '2302.05508', 'DBLP': 'journals/corr/abs-2302-05508', 'DOI': '10.48550/arXiv.2302.05508', 'CorpusId': 256826936}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 3/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** aaecd734-cc0c-4e36-8a57-7edf21dc002f<br>**Similarity:** 0.8467030142524171<br>**Text:** Source 3:\n",
       "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models As the ...<br>**Metadata:** {'title': 'Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '16d83e930a4dab2d49f5d276838ddce79df3f787', 'citationCount': 37, 'openAccessPdf': None, 'authors': ['Emilio Ferrara'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-03738', 'ArXiv': '2304.03738', 'DOI': '10.48550/arXiv.2304.03738', 'CorpusId': 258041203}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query the index\n",
    "query_string = \"list the methods to mitigate biases in large language models\"\n",
    "# query the citation query engine\n",
    "response = query_engine_default.query(query_string)\n",
    "display_response(\n",
    "    response, show_source=True, source_length=100, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Based on the provided sources, there are several methods proposed to mitigate biases in large language models:\n",
       "\n",
       "1. Data preprocessing: This involves cleaning and preprocessing the training data to remove any bias-related content before feeding it into the model. Source 2 provides a comprehensive evaluation of different kinds of biases exhibited by popular pretrained language models such as BERT, GPT-2, etc.\n",
       "2. Fairness metrics: Developing fairness metrics that can quantify and identify biases in large language models is crucial. Source 1 provides a multi-disciplinary approach to address the ethical implications of biases in generative language models.\n",
       "3. Debiasing techniques: Proposed methods include adversarial training, reweighting, and calibration to mitigate biases in large language models. Source 2 presents a toolkit that provides plug-and-play interfaces to connect mathematical tools for identifying biases with large pretrained language models such as BERT, GPT-2, etc.\n",
       "4. Anxiety induction: Inducing anxiety in large language models can increase exploration and bias. Source 3 demonstrates that emotion-inducing prompts can predictably change a model's behavior in cognitive tasks measuring exploratory decision-making and biases such as racism and ableism.\n",
       "5. Regularization techniques: This involves adding regularization terms to the model's objective function to penalize bias-related content. Source 1 discusses debiasing methods that can be applied to large language models, including adversarial training, reweighting, calibration, and regularization.\n",
       "6. Domain adaptation: This involves adapting a model trained on one domain to perform well on another domain with different biases. Source 2 provides a comprehensive evaluation of different kinds of biases exhibited by popular pretrained language models such as BERT, GPT-2, etc.\n",
       "7. Diverse and representative training data: Using diverse and representative training data can help mitigate biases in large language models. Source 1 discusses debiasing methods that can be applied to large language models, including adversarial training, reweighting, calibration, and regularization.\n",
       "8. Multi-task learning: Training a single model on multiple tasks simultaneously can help reduce biases by forcing the model to learn task-invariant representations. Source 3 demonstrates that emotion-inducing prompts can predictably change a model's behavior in cognitive tasks measuring exploratory decision-making and biases such as racism and ableism.\n",
       "9. Model interpretability: Interpreting the decisions made by large language models can help identify potential biases and take corrective action. Source 1 discusses debiasing methods that can be applied to large language models, including adversarial training, reweighting, calibration, and regularization.\n",
       "10. Human oversight: Having human oversight and review of the output generated by large language models can help identify potential biases and take corrective action. Source 2 provides a comprehensive evaluation of different kinds of biases exhibited by popular pretrained language models such as BERT, GPT-2, etc."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 1/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** aaecd734-cc0c-4e36-8a57-7edf21dc002f<br>**Similarity:** 6.705503786754444<br>**Text:** Source 1:\n",
       "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models As the ...<br>**Metadata:** {'title': 'Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '16d83e930a4dab2d49f5d276838ddce79df3f787', 'citationCount': 37, 'openAccessPdf': None, 'authors': ['Emilio Ferrara'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-03738', 'ArXiv': '2304.03738', 'DOI': '10.48550/arXiv.2304.03738', 'CorpusId': 258041203}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 2/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 670e58cd-d702-4a24-8a0d-85c14eed18e5<br>**Similarity:** 6.4996745211715<br>**Text:** Source 2:\n",
       "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Languag...<br>**Metadata:** {'title': 'FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '785bd5915f83f941d36c7996a9742ae695880111', 'citationCount': 2, 'openAccessPdf': None, 'authors': ['Hrishikesh Viswanath', 'Tianyi Zhang'], 'externalIds': {'ArXiv': '2302.05508', 'DBLP': 'journals/corr/abs-2302-05508', 'DOI': '10.48550/arXiv.2302.05508', 'CorpusId': 256826936}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**`Source Node 3/3`**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 87df8718-ae81-4a09-ab37-8e37db84e241<br>**Similarity:** 5.256617931405003<br>**Text:** Source 3:\n",
       "Inducing anxiety in large language models increases exploration and bias Large language...<br>**Metadata:** {'title': 'Inducing anxiety in large language models increases exploration and bias', 'venue': 'arXiv.org', 'year': 2023, 'paperId': '27c16cca907aa43397cc226a182b73b396c5cf66', 'citationCount': 14, 'openAccessPdf': None, 'authors': ['Julian Coda-Forno', 'K. Witte', 'A. Jagadish', 'Marcel Binz', 'Zeynep Akata', 'Eric Schulz'], 'externalIds': {'DBLP': 'journals/corr/abs-2304-11111', 'ArXiv': '2304.11111', 'DOI': '10.48550/arXiv.2304.11111', 'CorpusId': 258291914}}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine_bm25.query(query_string)\n",
    "display_response(\n",
    "    response, show_source=True, source_length=100, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion is that the bm25 retriever quality is better but it is slower"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
